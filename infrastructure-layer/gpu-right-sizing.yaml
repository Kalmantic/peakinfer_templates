id: "gpu-right-sizing"
name: "GPU Right-Sizing and Allocation Optimization"
description: "Optimize GPU allocation by matching workload requirements to appropriate GPU types and reducing over-provisioning"
category: "hardware_optimization"
confidence: 0.90
success_count: 1123
verified_environments: 56
contributors: ["gpu_specialist", "infra_optimizer", "cost_analyst"]
last_updated: "2025-01-14"

environment_match:
  gpu_utilization: "<50%"
  gpu_memory_utilization: "<60%"
  overprovisioned: true
  model_size: ["<7B", "7B", "13B"]

optimization:
  technique: "gpu_right_sizing"
  expected_cost_reduction: "40-60%"
  expected_performance_impact: "<5%"
  effort_estimate: "1-2 weeks"
  risk_level: "low"

economics:
  baseline_calculation:
    current_gpu_type: "A100-80GB"
    current_hourly_cost: 4.10
    current_gpu_count: "${gpu_count}"
    monthly_cost: "${current_hourly_cost} * 720 * ${current_gpu_count}"
  projected_improvement:
    optimized_gpu_type: "A10G"
    optimized_hourly_cost: 1.21
    optimized_gpu_count: "${gpu_count}"
    monthly_cost_optimized: "${optimized_hourly_cost} * 720 * ${optimized_gpu_count}"
  implementation_cost:
    engineering_hours: 60
    hourly_rate: 200
    total_cost: 12000

implementation:
  prerequisites:
    - requirement: "GPU utilization metrics"
      validation_command: "nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv"
    - requirement: "Model memory profiling"
      validation_command: "python scripts/profile_model_memory.py --model ${model_name}"
    - requirement: "Target GPU availability"
      validation_command: "aws ec2 describe-instance-type-offerings --filters Name=instance-type,Values=${target_instance}"

  automated_steps:
    - step_id: "workload_analysis"
      name: "Workload Analysis"
      executable: true
      commands:
        - "python scripts/analyze_gpu_usage.py --duration 7d --output gpu_analysis.json"
        - "python scripts/recommend_gpu_type.py --analysis gpu_analysis.json --model ${model_name}"
      validation:
        command: "python scripts/validate_recommendation.py"
        success_criteria: "confidence_score > 0.85"
        rollback_command: "echo 'Keep current configuration'"

    - step_id: "parallel_deployment"
      name: "Deploy on Right-Sized GPU"
      executable: true
      commands:
        - "python scripts/deploy_model.py --gpu-type ${recommended_gpu} --model ${model_name}"
        - "python scripts/run_comparison_benchmark.py --baseline ${current_endpoint} --candidate ${new_endpoint}"
      validation:
        command: "python scripts/compare_performance.py"
        success_criteria: "latency_degradation < 0.1 AND throughput_degradation < 0.1"
        rollback_command: "python scripts/terminate_candidate.py"

    - step_id: "traffic_migration"
      name: "Traffic Migration"
      executable: true
      commands:
        - "python scripts/migrate_traffic.py --from ${current_endpoint} --to ${new_endpoint} --canary-percent 10"
        - "python scripts/monitor_canary.py --duration 3600"
      validation:
        command: "python scripts/validate_canary.py"
        success_criteria: "error_rate < 0.01 AND latency_p99 within 10%"
        rollback_command: "python scripts/rollback_traffic.py"

monitoring:
  key_metrics:
    - metric: "gpu_utilization"
      target: ">60%"
      alert_threshold: "<40%"
    - metric: "gpu_memory_utilization"
      target: ">50%"
      alert_threshold: "<30%"
    - metric: "cost_per_inference"
      target: "<${baseline_cost} * 0.5"
      alert_threshold: ">${baseline_cost} * 0.7"
    - metric: "latency_p99"
      target: "<${baseline_latency} * 1.1"
      alert_threshold: ">${baseline_latency} * 1.3"

  rollback_triggers:
    - condition: "latency_p99 > baseline * 1.5 for 15 minutes"
      action: "automatic_rollback"
    - condition: "error_rate > 0.02 for 10 minutes"
      action: "automatic_rollback"
    - condition: "gpu_memory_utilization > 95% for 5 minutes"
      action: "alert_and_investigation"

results:
  recent_implementations:
    - environment: "chatbot_inference_service"
      baseline_gpu: "A100-80GB"
      optimized_gpu: "A10G"
      baseline_monthly_cost: 35000
      optimized_monthly_cost: 15400
      cost_reduction_percent: 56
      latency_impact_percent: 3
      implementation_days: 7
